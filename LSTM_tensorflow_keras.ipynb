{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MainLSTM(Model):\n",
    "    def __init__(self, dropout=0.0):\n",
    "        super(MainLSTM, self).__init__()\n",
    "        \n",
    "        # Masking layer to ignore missing values (-99)\n",
    "        self.mask = layers.Masking(mask_value=-99.0)\n",
    "\n",
    "        # LSTM for auxiliary stations\n",
    "        self.aux_lstm = layers.LSTM(8, return_sequences=True, dropout=dropout)\n",
    "        \n",
    "        # Main LSTM for concatenated sequence\n",
    "        self.station_lstm = layers.LSTM(128, return_sequences=True, dropout=dropout, \n",
    "                                        recurrent_dropout=0.0)\n",
    "        \n",
    "        # Fully-connected output\n",
    "        self.station_fc = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch_size, time_steps, features)\n",
    "        \n",
    "        # Apply masking\n",
    "        x = self.mask(inputs)\n",
    "        \n",
    "        # Split main vs auxiliary features\n",
    "        main_station_data = x[:, :, :10]  # First 10 features\n",
    "        aux_station_data = x[:, :, 10:]   # Remaining features\n",
    "        \n",
    "        # Process auxiliary stations in chunks of 8 features\n",
    "        aux_lstm_outs = []\n",
    "        for i in range(0, aux_station_data.shape[2], 8):\n",
    "            aux_chunk = aux_station_data[:, :, i:i+8]\n",
    "            aux_lstm_out = self.aux_lstm(aux_chunk)\n",
    "            aux_lstm_outs.append(aux_lstm_out)\n",
    "        \n",
    "        # Concatenate main station data + all aux LSTM outputs along features axis\n",
    "        concat = tf.concat([main_station_data] + aux_lstm_outs, axis=-1)\n",
    "        \n",
    "        # Pass through main LSTM\n",
    "        lstm_out = self.station_lstm(concat)\n",
    "        \n",
    "        # Take last time step\n",
    "        final_feature = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully-connected layer to predict GHI\n",
    "        ghi_pred = self.station_fc(final_feature)\n",
    "        \n",
    "        return ghi_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e030791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose X_train shape = (batch_size, 24, 34)\n",
    "model = MainLSTM(dropout=0.1)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GHI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
