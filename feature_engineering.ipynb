{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7221bfa6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7be0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob  # For loading multiple files\n",
    "\n",
    "# import random\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from plotting import plot_stations_matplotlib\n",
    "\n",
    "# from processAllData import getAllReadyForStationByLatAndLongAndK, getAllReadyForStationByLatAndLongAndKSplitTestAndTrain\n",
    "# from LSTMArchitecture import GHIDataset, Main_LSTM\n",
    "from constants import COLUMN_NAMES, USECOLS_NON_CLOUD, DTYPE_NON_CLOUD\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e479497",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e770f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(data):\n",
    "    \"\"\"\n",
    "    Converts wind speed/direction into vector components,\n",
    "    computes distance and alignment to station.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wind vector components\n",
    "    data[\"wind_u\"] = (\n",
    "        data[COLUMN_NAMES[\"WIND_SPEED\"]] * data[COLUMN_NAMES[\"WIND_DIRECTION_COS\"]]\n",
    "    )\n",
    "    data[\"wind_v\"] = (\n",
    "        data[COLUMN_NAMES[\"WIND_SPEED\"]] * data[COLUMN_NAMES[\"WIND_DIRECTION_SIN\"]]\n",
    "    )\n",
    "\n",
    "    # Distance from sensor to station\n",
    "    data[\"distance\"] = np.sqrt(data[\"distanceX\"] ** 2 + data[\"distanceY\"] ** 2)\n",
    "\n",
    "    # Alignment: how much wind points toward the station\n",
    "    dot = data[\"wind_u\"] * data[\"distanceX\"] + data[\"wind_v\"] * data[\"distanceY\"]\n",
    "    norm_wind = np.sqrt(data[\"wind_u\"] ** 2 + data[\"wind_v\"] ** 2)\n",
    "    norm_station = np.sqrt(data[\"distanceX\"] ** 2 + data[\"distanceY\"] ** 2)\n",
    "    data[\"alignment\"] = dot / (\n",
    "        norm_wind * norm_station + 1e-6\n",
    "    )  # epsilon to avoid div by 0\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_features(data):\n",
    "    features = [COLUMN_NAMES[\"WIND_SPEED\"], \"wind_u\", \"wind_v\", \"distance\", \"alignment\"]\n",
    "    X = data[features]\n",
    "    y = data[COLUMN_NAMES[\"GHI\"]]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_distance_model(X_train, y_train):\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=200, learning_rate=0.1, max_depth=4, random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_distance_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"RMSE on test set: {rmse:.2f} W/m^2\")\n",
    "\n",
    "    # Feature importance\n",
    "    importances = pd.Series(\n",
    "        model.feature_importances_, index=X_test.columns\n",
    "    ).sort_values(ascending=False)\n",
    "    print(\"Feature importances:\")\n",
    "    print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5decc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob  # For loading multiple files\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# from plotting import plot_stations_matplotlib\n",
    "from constants import COLUMN_NAMES, DTYPE_NON_CLOUD, USECOLS_NON_CLOUD\n",
    "\n",
    "\n",
    "def getStationTimePeriodFromYears(csv_file):\n",
    "    match = re.search(r\"(\\d{4})-(\\d{4})\", csv_file)\n",
    "    if match:\n",
    "        start_year = int(match.group(1))\n",
    "        end_year = int(match.group(2))\n",
    "        # print(start_year, end_year)  # Output: 2005 2017\n",
    "        return (start_year, end_year)\n",
    "\n",
    "\n",
    "def denormalizeHourOrMonth(anglesin, anglecos, cycleLength):\n",
    "    angle = np.arctan2(anglesin, anglecos)\n",
    "    cycleBeforeModulo = (angle / (2 * np.pi)) * cycleLength + 1\n",
    "    return cycleBeforeModulo % cycleLength\n",
    "\n",
    "\n",
    "# Euclidean distance function\n",
    "def euclidean_distance(lat1, lon1, lat2, lon2):\n",
    "    distance_vector = ((lat2 - lat1), lon2 - lon1)\n",
    "    return ((np.sqrt((lat2 - lat1) ** 2) + (lon2 - lon1) ** 2), distance_vector)\n",
    "\n",
    "\n",
    "# Function to find the K nearest stations (not returning nearest station given)\n",
    "def find_k_nearest_stations(df, station_name, k):\n",
    "    if station_name not in df[\"station\"].values:\n",
    "        return f\"Station {station_name} not found in the dataset.\"\n",
    "\n",
    "    # Get the coordinates of the given station\n",
    "    station_coords = df[df[\"station\"] == station_name][\n",
    "        [\"Latitude\", \"Longitude\"]\n",
    "    ].values[0]\n",
    "    lat1, lon1 = station_coords\n",
    "\n",
    "    # Calculate the distance from the given station to all other stations\n",
    "    df[\"distance\"] = df.apply(\n",
    "        lambda row: euclidean_distance(lat1, lon1, row[\"Latitude\"], row[\"Longitude\"]),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # station_coords = df[df['station'] == station_name][['norm_lat', 'norm_long']].values[0]\n",
    "    # lat1, lon1 = station_coords\n",
    "\n",
    "    # # Calculate the distance from the given station to all other stations\n",
    "    # df['distance'] = df.apply(lambda row: euclidean_distance(lat1, lon1, row['norm_lat'], row['norm_long']), axis=1)\n",
    "\n",
    "    # Sort the DataFrame by distance and get the K nearest stations\n",
    "    # nearest_stations = df[df['station'] != station_name].sort_values(by='distance').head(k)\n",
    "    nearest_stations = (\n",
    "        df[df[\"station\"] != station_name]\n",
    "        .sort_values(by=\"distance\", key=lambda x: x.apply(lambda y: y[0]))\n",
    "        .head(k)\n",
    "    )\n",
    "\n",
    "    return nearest_stations[\n",
    "        [\"station\", \"norm_lat\", \"norm_long\", \"distance\", \"StartTime\", \"EndTime\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "def latlon_to_xy(lat, lon, lat0):\n",
    "    \"\"\"Simple equirectangular projection.\"\"\"\n",
    "    R = 6371  # radius in km\n",
    "    x = R * np.radians(lon) * np.cos(np.radians(lat0))\n",
    "    y = R * np.radians(lat)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def compute_angle(vec1, vec2):\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0  # Could return np.pi to maximize diversity if desired\n",
    "    unit1 = vec1 / norm1\n",
    "    unit2 = vec2 / norm2\n",
    "    return np.arccos(np.clip(np.dot(unit1, unit2), -1.0, 1.0))\n",
    "\n",
    "\n",
    "def makeDistanceTuple(distance, x, y):\n",
    "    return (distance, (x, y))\n",
    "\n",
    "\n",
    "def getEastWestNorthSouthOrder(result_df):\n",
    "    ordered_rows = []\n",
    "\n",
    "    # Make a copy so we can remove selected stations\n",
    "    df_copy = result_df.copy()\n",
    "\n",
    "    # 1. Most East\n",
    "    east_idx = df_copy[\"x\"].idxmax()\n",
    "    ordered_rows.append(df_copy.loc[east_idx])\n",
    "    df_copy = df_copy.drop(east_idx)\n",
    "\n",
    "    # 2. Most West\n",
    "    west_idx = df_copy[\"x\"].idxmin()\n",
    "    ordered_rows.append(df_copy.loc[west_idx])\n",
    "    df_copy = df_copy.drop(west_idx)\n",
    "\n",
    "    # 3. Most North\n",
    "    north_idx = df_copy[\"y\"].idxmax()\n",
    "    ordered_rows.append(df_copy.loc[north_idx])\n",
    "    df_copy = df_copy.drop(north_idx)\n",
    "\n",
    "    # 4. Most South\n",
    "    if not df_copy.empty:\n",
    "        south_idx = df_copy[\"y\"].idxmin()\n",
    "        ordered_rows.append(df_copy.loc[south_idx])\n",
    "        df_copy = df_copy.drop(south_idx)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    return pd.DataFrame(ordered_rows).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def spatially_diverse_knn(df, station_name, k=3, candidate_pool=78):\n",
    "    # spatially_diverse_knn_df\n",
    "    #\n",
    "    # This function selects the 'k' nearest stations to a given target station,\n",
    "    # while ensuring that the selected neighbors are spatially diverse. Instead\n",
    "    # of simply choosing the closest stations, it incorporates an angular diversity\n",
    "    # criterion that selects stations spread out in different directions. This helps\n",
    "    # avoid picking stations that are clustered in one region around the target.\n",
    "    #\n",
    "    # Parameters:\n",
    "    #   - df: DataFrame containing station information (latitude, longitude, station_name)\n",
    "    #   - center_latlon: Coordinates (latitude, longitude) of the target station\n",
    "    #   - k: Number of neighbors to select (default is 3)\n",
    "    #   - candidate_pool: Number of candidates to consider before selection (default is len(csv_files)=78)\n",
    "    #\n",
    "    # Returns:\n",
    "    #   - A DataFrame with 'k' stations selected based on spatial diversity from the target\n",
    "    #   - The coordinates of the target station\n",
    "    # Exclude the center point if it exists in the set\n",
    "    if station_name not in df[\"station\"].values:\n",
    "        return f\"Station {station_name} not found in the dataset.\"\n",
    "\n",
    "    center_latlon = df[df[\"station\"] == station_name][[\"Latitude\", \"Longitude\"]].values[\n",
    "        0\n",
    "    ]\n",
    "    mask = ~(\n",
    "        (df[\"Latitude\"] == center_latlon[0]) & (df[\"Longitude\"] == center_latlon[1])\n",
    "    )\n",
    "    df_filtered = df[mask].copy()\n",
    "    # df_filtered['distance_vector'] = [(0.0, (0.0, 0.0))] * len(df_filtered)  # Initialize with default values\n",
    "\n",
    "    # Convert to x/y coords\n",
    "    lat0 = center_latlon[0]\n",
    "    df_filtered[\"x\"], df_filtered[\"y\"] = latlon_to_xy(\n",
    "        df_filtered[\"Latitude\"], df_filtered[\"Longitude\"], lat0\n",
    "    )\n",
    "    center_x, center_y = latlon_to_xy(center_latlon[0], center_latlon[1], lat0)\n",
    "    center_xy = np.array([center_x, center_y])\n",
    "\n",
    "    # Compute distances\n",
    "    df_filtered[\"distance\"] = np.linalg.norm(\n",
    "        df_filtered[[\"x\", \"y\"]].values - center_xy, axis=1\n",
    "    )\n",
    "    df_sorted = df_filtered.sort_values(\"distance\").head(candidate_pool)\n",
    "    df_sorted[\"distance\"] = df_sorted.apply(\n",
    "        lambda row: makeDistanceTuple(row[\"distance\"], row[\"x\"], row[\"y\"]), axis=1\n",
    "    )\n",
    "    # print(\"After sorting by distance\")\n",
    "    # print(df_sorted.head())\n",
    "    selected_rows = []\n",
    "    selected_xy = []\n",
    "\n",
    "    for idx, row in df_sorted.iterrows():\n",
    "        cand_xy = np.array([row[\"distance\"][1][0], row[\"distance\"][1][1]])\n",
    "        if len(selected_xy) == 0:  # gets closest station\n",
    "            selected_rows.append(row)\n",
    "            selected_xy.append(cand_xy)\n",
    "        else:\n",
    "            angles = [\n",
    "                compute_angle(cand_xy - center_xy, s - center_xy) for s in selected_xy\n",
    "            ]\n",
    "            min_angle = min(angles)\n",
    "            if min_angle > np.radians(45) or len(selected_xy) < k // 2:\n",
    "                selected_rows.append(row)\n",
    "                selected_xy.append(cand_xy)\n",
    "        if len(selected_rows) == k:\n",
    "            break\n",
    "\n",
    "    # Order by East -> West -> North/South\n",
    "    # Convert selected_rows to DataFrame\n",
    "    result_df = pd.DataFrame(selected_rows).reset_index(drop=True)\n",
    "    result_df = getEastWestNorthSouthOrder(result_df)\n",
    "\n",
    "    return result_df, center_latlon\n",
    "\n",
    "\n",
    "def find_nearest_station_given_long_lat(df, Longitude, Latitude):\n",
    "    # Find the station with the smallest Euclidean distance to the given coordinates\n",
    "    df[\"distance\"] = df.apply(\n",
    "        lambda row: euclidean_distance(\n",
    "            Latitude, Longitude, row[\"Latitude\"], row[\"Longitude\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    nearest_station = df.sort_values(\n",
    "        by=\"distance\", key=lambda x: x.apply(lambda y: y[0])\n",
    "    ).head(1)\n",
    "    return nearest_station[\n",
    "        [\"station\", \"Latitude\", \"Longitude\", \"distance\", \"StartTime\", \"EndTime\"]\n",
    "    ]\n",
    "\n",
    "    # To get station name print(str(wanted_station.station.iloc[0]))\n",
    "\n",
    "\n",
    "def find_nearest_station_given_normed_long_lat(df, Longitude_Normed, Latitude_Normed):\n",
    "    # Find the station with the smallest Euclidean distance to the given coordinates\n",
    "    df[\"distance\"] = df.apply(\n",
    "        lambda row: euclidean_distance(\n",
    "            Longitude_Normed, Latitude_Normed, row[\"norm_lat\"], row[\"norm_long\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    nearest_station = df.sort_values(\n",
    "        by=\"distance\", key=lambda x: x.apply(lambda y: y[0])\n",
    "    ).head(1)\n",
    "    return nearest_station[\n",
    "        [\"station\", \"norm_lat\", \"norm_long\", \"distance\", \"StartTime\", \"EndTime\"]\n",
    "    ]\n",
    "\n",
    "    # To get station name print(str(wanted_station.station.iloc[0]))\n",
    "\n",
    "\n",
    "def modify_nearest_stations(nearest_stations):\n",
    "    modified_nearest_stations = []\n",
    "    for station in nearest_stations[\"station\"]:\n",
    "        modified_station = station.replace(\" \", \"-\")\n",
    "        modified_nearest_stations.append(modified_station)\n",
    "    return modified_nearest_stations\n",
    "\n",
    "\n",
    "def find_stations_csv(stations, csvs):\n",
    "    station_order = []\n",
    "    nearest_stations_csvs = []\n",
    "    for station in stations:\n",
    "        for csv in csvs:\n",
    "            if station.lower().replace(\"-\", \"_\") in csv.lower().replace(\"-\", \"_\"):\n",
    "                nearest_stations_csvs.append(csv)\n",
    "                print(f\"Station {station} found in {csv}\")\n",
    "                station_order.append(station)\n",
    "                break\n",
    "    return nearest_stations_csvs, station_order\n",
    "\n",
    "\n",
    "def adjustWindDirection(wind_direction_degress, theta_relative):\n",
    "    # Adjusts auxillary stations wind direction to be relative to the wanted stations position\n",
    "    # Adjust the wind direction by subtracting the relative angle\n",
    "    adjusted_wind_direction = wind_direction_degress - theta_relative\n",
    "    # Normalize to 0-360 degrees\n",
    "    adjusted_wind_direction = (adjusted_wind_direction + 360) % 360\n",
    "    return adjusted_wind_direction\n",
    "\n",
    "\n",
    "def cyclicalEncoding(data, cycleLength):\n",
    "    newDatasin = np.sin(\n",
    "        2 * np.pi * (data - 1) / cycleLength\n",
    "    )  # Sine encoding for hours (adjust for 0-23)\n",
    "    newDatacos = np.cos(\n",
    "        2 * np.pi * (data - 1) / cycleLength\n",
    "    )  # Cosine encoding for hours (adjust for 0-23)\n",
    "    return newDatasin, newDatacos\n",
    "\n",
    "\n",
    "def process_data(\n",
    "    df,\n",
    "    usecols,\n",
    "    min_start_year,\n",
    "    max_end_year,\n",
    "    i,\n",
    "    wantedStationCSV=False,\n",
    "    RelativeAnglesDegrees=[(0, 0)],\n",
    "):\n",
    "    for col in usecols:\n",
    "        match col:\n",
    "            case x if x == COLUMN_NAMES[\"YEAR_MONTH_DAY_HOUR\"]:\n",
    "                # Convert columns to numeric, coercing errors to NaN\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").replace(99, np.nan)\n",
    "                df = df[df[col].between(min_start_year, max_end_year)]\n",
    "                df = df.drop(columns=[col], axis=1)\n",
    "\n",
    "            case x if x == COLUMN_NAMES[\"OPAQUE_SKY_COVER\"]:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").replace(99, np.nan)\n",
    "\n",
    "            case x if x in [COLUMN_NAMES[\"MONTH\"], COLUMN_NAMES[\"HOUR\"]]:\n",
    "                df = df.drop(columns=[col], axis=1)\n",
    "\n",
    "            case x if x in [\n",
    "                COLUMN_NAMES[\"HOUR_SIN\"],\n",
    "                COLUMN_NAMES[\"HOUR_COS\"],\n",
    "                COLUMN_NAMES[\"MONTH_SIN\"],\n",
    "                COLUMN_NAMES[\"MONTH_COS\"],\n",
    "            ]:\n",
    "                if not wantedStationCSV:\n",
    "                    df = df.drop(columns=[col], axis=1)\n",
    "                else:\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\").replace(\n",
    "                        99, np.nan\n",
    "                    )\n",
    "\n",
    "            case x if x in [\n",
    "                COLUMN_NAMES[\"WIND_DIRECTION_SIN\"],\n",
    "                COLUMN_NAMES[\"WIND_DIRECTION_COS\"],\n",
    "            ]:\n",
    "                if wantedStationCSV:\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\").replace(\n",
    "                        99, np.nan\n",
    "                    )\n",
    "\n",
    "            case x if x == COLUMN_NAMES[\"WIND_DIRECTION\"]:\n",
    "                if not wantedStationCSV:\n",
    "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\").replace(\n",
    "                        99, np.nan\n",
    "                    )\n",
    "                    df[col] = df[col].apply(\n",
    "                        lambda x: adjustWindDirection(x, RelativeAnglesDegrees[i])\n",
    "                    )\n",
    "                    radians = np.deg2rad(df[col])\n",
    "                    df[COLUMN_NAMES[\"WIND_DIRECTION_SIN\"]] = np.sin(radians)\n",
    "                    df[COLUMN_NAMES[\"WIND_DIRECTION_COS\"]] = np.cos(radians)\n",
    "                df = df.drop(columns=[col], axis=1)\n",
    "\n",
    "            case x if x == COLUMN_NAMES[\"WIND_SPEED\"]:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").replace(99, np.nan)\n",
    "\n",
    "            case x if x in [\n",
    "                COLUMN_NAMES[\"GHI\"],\n",
    "                COLUMN_NAMES[\"DNI\"],\n",
    "                COLUMN_NAMES[\"DHI\"],\n",
    "            ]:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\").replace(9999, np.nan)\n",
    "\n",
    "            case _:\n",
    "                df[col] = df.drop(columns=[col], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_nearest_stations_data(\n",
    "    nearest_stations_csvs,\n",
    "    min_start_year,\n",
    "    max_end_year,\n",
    "    wantedStationCSV=False,\n",
    "    RelativeAnglesDegrees=[(0, 0)],\n",
    "    usecols=[],\n",
    "    dtype={},\n",
    "):\n",
    "    dfs = []\n",
    "    i = 0\n",
    "    for f in nearest_stations_csvs:\n",
    "        # Read the CSV file with specified columns and data types\n",
    "        df = pd.read_csv(\n",
    "            f,\n",
    "            delimiter=\",\",\n",
    "            index_col=False,\n",
    "            usecols=usecols,\n",
    "            dtype=dtype,\n",
    "            on_bad_lines=\"skip\",\n",
    "        )\n",
    "\n",
    "        df = process_data(\n",
    "            df,\n",
    "            usecols,\n",
    "            min_start_year,\n",
    "            max_end_year,\n",
    "            i,\n",
    "            wantedStationCSV=wantedStationCSV,\n",
    "            RelativeAnglesDegrees=RelativeAnglesDegrees,\n",
    "        )\n",
    "\n",
    "        # print(df.dtypes)\n",
    "        df.fillna(0, inplace=True)\n",
    "        dfs.append(df)\n",
    "        i += 1\n",
    "        # print(df.columns)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def chunk(df, interval=25):\n",
    "    results = []\n",
    "    for i in range(0, len(df) - interval, 1):\n",
    "        chunk = df[i : i + interval]\n",
    "        results.append(chunk)\n",
    "    results = np.array(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_chunked_tensors(nearest_stations, dfs, interval):\n",
    "    chunked_tensors = []\n",
    "    rows = 0\n",
    "    station_order = []\n",
    "    for index, station in nearest_stations.iterrows():\n",
    "        distanceVector = station.iloc[3][1]\n",
    "        chunked_df = chunk(dfs[rows], interval=interval)\n",
    "        rows += 1\n",
    "        # print(\"chunked df shape:\", chunked_df.shape)\n",
    "        chunkedTensor = torch.tensor(chunked_df).to(torch.float32)\n",
    "        chunked_tensors.append(chunkedTensor)\n",
    "        station_order.append(station)\n",
    "    return chunked_tensors, station_order\n",
    "\n",
    "\n",
    "def getMaxStartMinEndYearComplete(max_start_year, min_end_year):\n",
    "    maxStartYear, minEndYear = str(max_start_year), str(min_end_year)\n",
    "    # print(max_start_year, min_end_year)\n",
    "    maxStartYear = maxStartYear + \"010100\"\n",
    "    minEndYear = minEndYear + \"123124\"\n",
    "    maxStartYear = int(maxStartYear)\n",
    "    minEndYear = int(minEndYear)\n",
    "    return maxStartYear, minEndYear\n",
    "\n",
    "\n",
    "# def angle_from_wanted_station(row, wanted_station_long, wanted_station_lat):\n",
    "#   print(row[\"station\"])\n",
    "#   lat, lon = row['Latitude'], row['Longitude']\n",
    "#   delta_lon = lon - wanted_station_long\n",
    "#   delta_lat = lat - wanted_station_lat\n",
    "#   angle = np.arctan2(delta_lat, delta_lon)  # Angle in radians\n",
    "#   angle_degrees = np.degrees(angle)\n",
    "#   if angle_degrees < 0:\n",
    "#     angle_degrees += 360  # Normalize to [0, 360]\n",
    "#   return angle_degrees\n",
    "\n",
    "# def sortAuxillaryStations(nearest_stations_df, wanted_station_lat, wanted_station_long):\n",
    "#   \"\"\"\n",
    "#   From the auxillary stations found, sort them to have the first one be the eastern most station, the second be the western most station the third be the northern most station and the fourth be the southern most station\n",
    "#   \"\"\"\n",
    "\n",
    "#   nearest_stations_df['angle'] = nearest_stations_df.apply(angle_from_wanted_station, axis=1, wanted_station_long=wanted_station_long, wanted_station_lat=wanted_station_lat)\n",
    "#   sorted_stations = nearest_stations_df.sort_values(by='angle').reset_index(drop=True)\n",
    "#   print(\"Sorted Stations by angle (0 degrees is east, 90 degrees is north, 180 degrees is west, 270 degrees is south):\")\n",
    "#   print(sorted_stations.head())\n",
    "#   return sorted_stations\n",
    "\n",
    "\n",
    "def normalize_column(column: str, wanted_df: pd.DataFrame, aux_dfs: list[pd.DataFrame]):\n",
    "    # Calculate mean and std across all dataframes. Can use mean of means as they all have same length\n",
    "    wanted_mean = wanted_df[column].mean()\n",
    "    wanted_std = wanted_df[column].std()\n",
    "\n",
    "    aux_means = [df[column].mean() for df in aux_dfs]\n",
    "    aux_stds = [df[column].std() for df in aux_dfs]\n",
    "\n",
    "    mean = (wanted_mean + sum(aux_means)) / (1 + len(aux_means))\n",
    "    std = (wanted_std + sum(aux_stds)) / (1 + len(aux_stds))\n",
    "\n",
    "    # Standardized Normalization\n",
    "    # Normalizing wanted station\n",
    "    wanted_df[column] = (wanted_df[column] - mean) / std\n",
    "    # Normalizing auxillary stations\n",
    "    for df in aux_dfs:\n",
    "        df[column] = (df[column] - mean) / std\n",
    "\n",
    "    return wanted_df, aux_dfs, mean, std\n",
    "\n",
    "\n",
    "columns_to_normalize = [\n",
    "    COLUMN_NAMES[\"GHI\"],\n",
    "    COLUMN_NAMES[\"DNI\"],\n",
    "    COLUMN_NAMES[\"DHI\"],\n",
    "    COLUMN_NAMES[\"WIND_SPEED\"],\n",
    "]\n",
    "\n",
    "\n",
    "def normalize_dataframes(wanted_df: pd.DataFrame, aux_dfs: list[pd.DataFrame]):\n",
    "    meanGHI = 0\n",
    "    stdGHI = 0\n",
    "    for column in columns_to_normalize:\n",
    "        wanted_df, aux_dfs, mean, std = normalize_column(column, wanted_df, aux_dfs)\n",
    "        print(f\"Normalized {column} with mean: {mean}, std: {std}\")\n",
    "        if column == COLUMN_NAMES[\"GHI\"]:\n",
    "            meanGHI = mean\n",
    "            stdGHI = std\n",
    "\n",
    "    return [wanted_df], aux_dfs, meanGHI, stdGHI\n",
    "\n",
    "\n",
    "def get_relative_angle_degrees(nearest_stations):\n",
    "    RelativeAnglesDegrees = []\n",
    "    for distanceVector in nearest_stations[\"distance\"].values:\n",
    "        Relative_Angle_Radians = np.arctan2(\n",
    "            distanceVector[1][1], distanceVector[1][0]\n",
    "        )  # Relative Distance Vector y and Distance Vector x\n",
    "        Relative_Angle_Degrees = np.degrees(Relative_Angle_Radians)\n",
    "        if Relative_Angle_Degrees < 0:\n",
    "            Relative_Angle_Degrees += 360\n",
    "        RelativeAnglesDegrees.append(Relative_Angle_Degrees)\n",
    "    return RelativeAnglesDegrees\n",
    "\n",
    "\n",
    "def getAllKAuxillaryStationsReadyByWantedStationName(\n",
    "    stationsName_lat_long_datadf,\n",
    "    nearest_stations,\n",
    "    k,\n",
    "    min_start_year,\n",
    "    max_end_year,\n",
    "    RelativeAnglesDegrees,\n",
    "    csv_files,\n",
    "    usecols=[],\n",
    "    dtype={},\n",
    "):\n",
    "    # doing all thats needed to get Auxillary stations data ready and in chunked tensors and in order chunked\n",
    "    modified_nearest_stations = modify_nearest_stations(nearest_stations)\n",
    "    nearest_stations_csvs, station_order = find_stations_csv(\n",
    "        modified_nearest_stations, csv_files\n",
    "    )\n",
    "    nearest_stations_data_dfs = get_nearest_stations_data(\n",
    "        nearest_stations_csvs=nearest_stations_csvs,\n",
    "        min_start_year=min_start_year,\n",
    "        max_end_year=max_end_year,\n",
    "        wantedStationCSV=False,\n",
    "        RelativeAnglesDegrees=RelativeAnglesDegrees,\n",
    "        usecols=usecols,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    for i in range(k):\n",
    "        nearest_stations_data_dfs[i][\"distanceX\"] = [\n",
    "            nearest_stations[\"distance\"].values[i][1][0]\n",
    "        ] * len(nearest_stations_data_dfs[i])\n",
    "        nearest_stations_data_dfs[i][\"distanceY\"] = [\n",
    "            nearest_stations[\"distance\"].values[i][1][1]\n",
    "        ] * len(nearest_stations_data_dfs[i])\n",
    "    return nearest_stations_data_dfs, nearest_stations_data_dfs\n",
    "\n",
    "\n",
    "def getAllReadyForStationByLatAndLongAndK(\n",
    "    stationsName_lat_long_datadf, lat, long, k, csv_files, usecols=[], dtype={}\n",
    "):\n",
    "    try:\n",
    "        # doing everything needed to get wanted stations data ready and in chunked tensors as well as get wanted station and its name\n",
    "        wanted_station = find_nearest_station_given_long_lat(\n",
    "            stationsName_lat_long_datadf, lat, long\n",
    "        )\n",
    "        max_start_year = wanted_station[\"StartTime\"].values[0]\n",
    "        min_end_year = wanted_station[\"EndTime\"].values[0]\n",
    "\n",
    "        wanted_station.at[wanted_station.index[0], \"distance\"] = (0.0, (0.0, 0.0))\n",
    "        wanted_station_modified = modify_nearest_stations(\n",
    "            {\"station\": wanted_station[\"station\"]}\n",
    "        )\n",
    "        wanted_station_csv, _ = find_stations_csv(wanted_station_modified, csv_files)\n",
    "        nearest_stations, target_point = spatially_diverse_knn(\n",
    "            stationsName_lat_long_datadf, wanted_station[\"station\"].values[0], k\n",
    "        )\n",
    "\n",
    "        max_start_year, min_end_year = (\n",
    "            max(max_start_year, *nearest_stations[\"StartTime\"].values),\n",
    "            min(min_end_year, *nearest_stations[\"EndTime\"].values),\n",
    "        )\n",
    "        max_start_year, min_end_year = getMaxStartMinEndYearComplete(\n",
    "            max_start_year, min_end_year\n",
    "        )\n",
    "        print(max_start_year, min_end_year)\n",
    "\n",
    "        # Get Relative Angles in Degrees for Wind Direction Adjustment\n",
    "        RelativeAnglesDegrees = get_relative_angle_degrees(nearest_stations)\n",
    "\n",
    "        # Get Auxillary Stations DataFrames\n",
    "        nearest_stations_data_dfs, aux_stations_dfs = (\n",
    "            getAllKAuxillaryStationsReadyByWantedStationName(\n",
    "                stationsName_lat_long_datadf=stationsName_lat_long_datadf,\n",
    "                nearest_stations=nearest_stations,\n",
    "                k=k,\n",
    "                min_start_year=max_start_year,\n",
    "                max_end_year=min_end_year,\n",
    "                RelativeAnglesDegrees=RelativeAnglesDegrees,\n",
    "                csv_files=csv_files,\n",
    "                usecols=usecols,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Get Wanted Station DataFrame\n",
    "        wanted_station_data_dfs = get_nearest_stations_data(\n",
    "            wanted_station_csv,\n",
    "            max_start_year,\n",
    "            min_end_year,\n",
    "            wantedStationCSV=True,\n",
    "            usecols=usecols,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "        # Normalize all stations data.\n",
    "        wanted_station_data_dfs, aux_stations_dfs, meanGHI, stdGHI = (\n",
    "            normalize_dataframes(wanted_station_data_dfs[0], aux_stations_dfs)\n",
    "        )\n",
    "\n",
    "        # Get Auxillary Stations Chunked Tensors\n",
    "        aux_chunked_tensors, aux_chunked_station_order = get_chunked_tensors(\n",
    "            nearest_stations, nearest_stations_data_dfs, 25\n",
    "        )\n",
    "\n",
    "        # Get Wanted Station Chunked Tensors\n",
    "        wanted_chunked_tensors, _ = get_chunked_tensors(\n",
    "            wanted_station, wanted_station_data_dfs, 25\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            wanted_chunked_tensors,\n",
    "            wanted_station[\"station\"].values[0],\n",
    "            aux_chunked_tensors,\n",
    "            aux_chunked_station_order,\n",
    "            meanGHI,\n",
    "            stdGHI,\n",
    "            aux_stations_dfs,\n",
    "            wanted_station_data_dfs,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in getAllReadyForStationByLatAndLongAndK: \", e)\n",
    "        return None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "def getAllReadyForStationByLatAndLongAndKSplitTestAndTrain(\n",
    "    stationsName_lat_long_datadf, lat, long, k, csv_files, usecols=[], dtype={}\n",
    "):\n",
    "    # doing everything needed to get wanted stations data ready and in chunked tensors as well as get wanted station and its name\n",
    "    wanted_station = find_nearest_station_given_long_lat(\n",
    "        stationsName_lat_long_datadf, lat, long\n",
    "    )\n",
    "    max_start_year = wanted_station[\"StartTime\"].values[0]\n",
    "    min_end_year = wanted_station[\"EndTime\"].values[0]\n",
    "\n",
    "    wanted_station.at[wanted_station.index[0], \"distance\"] = (0.0, (0.0, 0.0))\n",
    "    wanted_station_modified = modify_nearest_stations(\n",
    "        {\"station\": wanted_station[\"station\"]}\n",
    "    )\n",
    "    wanted_station_csv, _ = find_stations_csv(wanted_station_modified, csv_files)\n",
    "\n",
    "    nearest_stations, target_point = spatially_diverse_knn(\n",
    "        stationsName_lat_long_datadf, wanted_station[\"station\"].values[0], k\n",
    "    )\n",
    "    # plot_stations_matplotlib(target_point, nearest_stations, stationsName_lat_long_datadf)\n",
    "    max_start_year, min_end_year = (\n",
    "        max(max_start_year, *nearest_stations[\"StartTime\"].values),\n",
    "        min(min_end_year, *nearest_stations[\"EndTime\"].values),\n",
    "    )  # Get earliest year that all start at (max_start_year) and latest year all go until (min_end_year)\n",
    "    train_max_start_year, train_min_end_year = getMaxStartMinEndYearComplete(\n",
    "        max_start_year, min_end_year - 2\n",
    "    )\n",
    "    test_max_start_year, test_min_end_year = getMaxStartMinEndYearComplete(\n",
    "        min_end_year - 1, min_end_year\n",
    "    )\n",
    "\n",
    "    print(max_start_year, min_end_year)\n",
    "    print(train_max_start_year, train_min_end_year)\n",
    "    print(test_max_start_year, test_min_end_year)\n",
    "\n",
    "    RelativeAnglesDegrees = []\n",
    "    for distanceVector in nearest_stations[\"distance\"].values:\n",
    "        Relative_Angle_Radians = np.arctan2(\n",
    "            distanceVector[1][1], distanceVector[1][0]\n",
    "        )  # Relative Distance Vector y and Distance Vector x\n",
    "        Relative_Angle_Degrees = np.degrees(Relative_Angle_Radians)\n",
    "        if Relative_Angle_Degrees < 0:\n",
    "            Relative_Angle_Degrees += 360\n",
    "        RelativeAnglesDegrees.append(Relative_Angle_Degrees)\n",
    "\n",
    "    trainSet_aux_chunked_tensors, trainSet_aux_chunked_station_order = (\n",
    "        getAllKAuxillaryStationsReadyByWantedStationName(\n",
    "            stationsName_lat_long_datadf,\n",
    "            nearest_stations,\n",
    "            k,\n",
    "            train_max_start_year,\n",
    "            train_min_end_year,\n",
    "            RelativeAnglesDegrees,\n",
    "            csv_files,\n",
    "            usecols=usecols,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "    )\n",
    "    testSet_aux_chunked_tensors, testSet_aux_chunked_station_order = (\n",
    "        getAllKAuxillaryStationsReadyByWantedStationName(\n",
    "            stationsName_lat_long_datadf,\n",
    "            nearest_stations,\n",
    "            k,\n",
    "            test_max_start_year,\n",
    "            test_min_end_year,\n",
    "            RelativeAnglesDegrees,\n",
    "            csv_files,\n",
    "            usecols=usecols,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    trainSet_wanted_station_data_dfs = get_nearest_stations_data(\n",
    "        wanted_station_csv,\n",
    "        train_max_start_year,\n",
    "        train_min_end_year,\n",
    "        wantedStationCSV=True,\n",
    "        usecols=usecols,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    testSet_wanted_station_data_dfs = get_nearest_stations_data(\n",
    "        wanted_station_csv,\n",
    "        test_max_start_year,\n",
    "        test_min_end_year,\n",
    "        wantedStationCSV=True,\n",
    "        usecols=usecols,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    # print(wanted_station_data_dfs[0].head())\n",
    "\n",
    "    trainSet_wanted_chunked_tensors, _ = get_chunked_tensors(\n",
    "        wanted_station, trainSet_wanted_station_data_dfs, 25\n",
    "    )\n",
    "    testSet_wanted_chunked_tensors, _ = get_chunked_tensors(\n",
    "        wanted_station, testSet_wanted_station_data_dfs, 25\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        trainSet_wanted_chunked_tensors,\n",
    "        testSet_wanted_chunked_tensors,\n",
    "        wanted_station[\"station\"].values[0],\n",
    "        trainSet_aux_chunked_tensors,\n",
    "        testSet_aux_chunked_tensors,\n",
    "        trainSet_aux_chunked_station_order,\n",
    "        testSet_aux_chunked_station_order,\n",
    "    )\n",
    "\n",
    "\n",
    "num_aux_stations = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913340d9",
   "metadata": {},
   "source": [
    "## model related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c666f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device):\n",
    "    # if(combined_test_chunked_data_tensor.shape):\n",
    "    #   dataset = GHIDataset(combined_test_chunked_data_tensor, device=device)\n",
    "    #   train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    # dataset = GHIDataset(combined_chunked_data_tensor, device=device)\n",
    "    # train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    model.eval()\n",
    "    outputlist = []\n",
    "    targetData = []\n",
    "    colourList = []\n",
    "    for inputs, targets in test_loader:\n",
    "        output = model(inputs)\n",
    "        colourList.append(\n",
    "            inputs[:, -1].detach().cpu().numpy()\n",
    "        )  # Saving hour before predicted hour\n",
    "        targetData.append(targets.detach().cpu().numpy())\n",
    "        outputlist.append(output.detach().cpu().numpy())\n",
    "    colourList = np.concatenate(\n",
    "        colourList[:-1], axis=0\n",
    "    )  # Last batch size is too small so getting rid of it with the -1\n",
    "    outputlist = np.concatenate(outputlist[:-1], axis=0)\n",
    "    targetData = np.concatenate(targetData[:-1], axis=0)\n",
    "    # del combined_chunked_data_tensor #Saving RAM space\n",
    "    return outputlist, targetData, colourList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532dde2",
   "metadata": {},
   "source": [
    "# Model Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GHIDataset(Dataset):\n",
    "    def __init__(self, chunked_data, device=\"cuda\"):\n",
    "        super(GHIDataset, self).__init__()\n",
    "        self.chunked_data = chunked_data.to(\n",
    "            device\n",
    "        )  # chunked data needs to be passed in as a tensor\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunked_data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        element = self.chunked_data[\n",
    "            i\n",
    "        ]  # need to break down to zeroth element of list and first element of that is GHI of that which is the target\n",
    "        # inputs = []\n",
    "        # for el in element:\n",
    "        #   inputs.append(el[0:-1])\n",
    "        # print(element.shape)\n",
    "        return element[0:-1], element[\n",
    "            -1, 0\n",
    "        ]  # ensure first station in dataset is the one we are wanting to predict\n",
    "\n",
    "\n",
    "class Main_LSTM(nn.Module):\n",
    "    def __init__(self, dropout=0.0, num_aux_stations=3):\n",
    "        \"\"\"\n",
    "        input_dim: Number of features per time step (here, 5)\n",
    "        hidden_dim: Number of hidden units in the LSTM\n",
    "        num_layers: Number of stacked LSTM layers\n",
    "        dropout: Dropout probability (applied between LSTM layers if num_layers > 1)\n",
    "        \"\"\"\n",
    "        super(Main_LSTM, self).__init__()\n",
    "        num_input = 10 + num_aux_stations * 8\n",
    "        self.Stationlstm = nn.LSTM(num_input, 128, 3, batch_first=True, dropout=dropout)\n",
    "        self.Auxlstm = nn.LSTM(\n",
    "            8, 8, 2, batch_first=True, dropout=dropout\n",
    "        )  # may need to change 16 back to 8\n",
    "        # A fully-connected layer to map the LSTM output to a single GHI prediction\n",
    "        self.station_fc = nn.Linear(128, 1)\n",
    "        self.num_aux_stations = num_aux_stations\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        # LSTM returns output for every time step, and the final hidden and cell states.\n",
    "\n",
    "        # torch.Size([32, 24, 64])\n",
    "        main_station_data = x[:, :, :10]  # First 10 features are for main station\n",
    "        aux_station_data = x[:, :, 10:]  # Remaining features are for auxiliary stations\n",
    "        aux_lstm_outs = []\n",
    "        # other idea: range start at 10 go to 34 add 8 each time as 3 auxillaries\n",
    "        for i in range(\n",
    "            0, self.num_aux_stations * 8, 8\n",
    "        ):  # range start at 0 go to 8*num_aux_stations add 8 each time as num_aux_stations auxillary stations\n",
    "            lstm_out, _ = self.Auxlstm(aux_station_data[:, :, i : i + 8])\n",
    "            aux_lstm_outs.append(lstm_out)\n",
    "        concat = torch.cat(([main_station_data] + aux_lstm_outs), 2)\n",
    "        # print(concat.shape)\n",
    "        lstm_out, (h_n, c_n) = self.Stationlstm(concat)\n",
    "        # Use the output from the last time step as a summary of the sequence\n",
    "        # lstm_out[:, -1, :] has shape (batch_size, hidden_dim)\n",
    "        # print(\"Main: \", lstm_out.shape)\n",
    "        final_feature = lstm_out[:, -1, :]  # Not sure if this WORKSSSS (Right size?)\n",
    "        # torch.Size([32, 64])\n",
    "\n",
    "        # Pass through the fully-connected layer to produce a single output\n",
    "        ghi_pred = self.station_fc(final_feature)  # shape: (batch_size, 1)\n",
    "        return ghi_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893837c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"  # torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "mainModel = Main_LSTM(num_aux_stations=num_aux_stations).to(device)\n",
    "mainModel = torch.compile(mainModel)\n",
    "# mainModel = Main_LSTM()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error is common for regression tasks\n",
    "optimizer_main = optim.Adam(mainModel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec3208",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a10aa743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station QUESNEL found in Datasets/CWEEDS_2020_BC_cleaned_non_cloud\\QUESNEL_no_sky_cover.csv\n",
      "1998010100 2017123124\n",
      "Station BLUE-RIVER-CS found in Datasets/CWEEDS_2020_BC_cleaned_non_cloud\\BLUE_RIVER_CS_no_sky_cover.csv\n",
      "Station PUNTZI-MOUNTAIN-(AUT) found in Datasets/CWEEDS_2020_BC_cleaned_non_cloud\\PUNTZI_MOUNTAIN_(AUT)_no_sky_cover.csv\n",
      "Station PRINCE-GEORGE found in Datasets/CWEEDS_2020_BC_cleaned_non_cloud\\PRINCE_GEORGE_no_sky_cover.csv\n",
      "Station WILLIAMS-LAKE-A found in Datasets/CWEEDS_2020_BC_cleaned_non_cloud\\WILLIAMS_LAKE_A_no_sky_cover.csv\n",
      "got auxillary stations data dfs\n",
      "<class 'list'> <class 'pandas.core.frame.DataFrame'>\n",
      "Normalized GHI_kJ/m2 with mean: 478.9616278804472, std: 753.7647783590521\n",
      "Normalized DNI_kJ/m2 with mean: 452.87493725758605, std: 847.7786302752787\n",
      "Normalized DHI_kJ/m2 with mean: 249.06477983116588, std: 356.09061815781496\n",
      "Normalized wind_speed with mean: 20.444437599817476, std: 20.070777452990047\n",
      "Normalized all dataframes\n",
      "<class 'list'> <class 'pandas.core.frame.DataFrame'> <class 'list'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n",
      "Chunking aux\n",
      "Chunked wanted station data\n",
      "combining\n"
     ]
    }
   ],
   "source": [
    "non_cloud_path = \"Datasets/CWEEDS_2020_BC_cleaned_non_cloud\"\n",
    "cloud_path = \"Datasets/CWEEDS_2020_BC_cleaned_cloud\"\n",
    "csv_files = glob.glob(os.path.join(non_cloud_path, \"*.csv\"))\n",
    "file_path = \"Datasets/stationsName_lat_long_data.csv\"\n",
    "stationsName_lat_long_datadf = pd.read_csv(\n",
    "    file_path, delimiter=\",\", on_bad_lines=\"skip\"\n",
    ")\n",
    "num_aux_stations = 4\n",
    "(\n",
    "    wanted_chunked_tensors,\n",
    "    wanted_station_name,\n",
    "    aux_chunked_tensors,\n",
    "    aux_chunked_station_order,\n",
    "    meanGHI1,\n",
    "    stdGHI1,\n",
    "    aux_stations_dfs,\n",
    "    wanted_station_data_dfs,\n",
    ") = getAllReadyForStationByLatAndLongAndK(\n",
    "    stationsName_lat_long_datadf.copy(),\n",
    "    lat=-122.5,\n",
    "    long=53,\n",
    "    k=num_aux_stations,\n",
    "    csv_files=csv_files,\n",
    "    usecols=USECOLS_NON_CLOUD,\n",
    "    dtype=DTYPE_NON_CLOUD,\n",
    ")\n",
    "print(\"combining\")\n",
    "combined_train_chunked_data_tensor = torch.cat(\n",
    "    wanted_chunked_tensors + aux_chunked_tensors, dim=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bd99c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([175295, 25, 42])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train_chunked_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045bcf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def analyze_auxiliary_stations(wanted_df, aux_list, lags=24, alpha=0.01, plot=True):\n",
    "    \"\"\"\n",
    "    Compare multiple auxiliary stations' wind features against wanted station GHI.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    wanted_df : pd.DataFrame\n",
    "        Must contain 'timestamp' and 'GHI_target'.\n",
    "    aux_list : list of pd.DataFrame\n",
    "        Each dataframe must contain 'timestamp', 'wind_speed', 'wind_direction'.\n",
    "    lags : int\n",
    "        Number of lag hours to test (default 24).\n",
    "    alpha : float\n",
    "        Regularization strength for Lasso when testing multi-lags.\n",
    "    plot : bool\n",
    "        If True, plots R² vs lag for each station.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Keys = station index, values = dict with:\n",
    "            - 'best_single_lag'\n",
    "            - 'lag_ranking' (DataFrame of lag & R²)\n",
    "            - 'best_multi_lags' (list from Lasso)\n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "    print(wanted_df.head())\n",
    "    for idx, aux_df in enumerate(aux_list):\n",
    "        print(aux_df.head())\n",
    "        # --- 1. Merge wanted + auxiliary station on timestamp ---\n",
    "        df = pd.merge(\n",
    "            wanted_df[[COLUMN_NAMES[\"YEAR_MONTH_DAY_HOUR\"], COLUMN_NAMES[\"GHI\"]]],\n",
    "            aux_df[\n",
    "                [\n",
    "                    COLUMN_NAMES[\"YEAR_MONTH_DAY_HOUR\"],\n",
    "                    COLUMN_NAMES[\"WIND_SPEED\"],\n",
    "                    COLUMN_NAMES[\"WIND_DIRECTION\"],\n",
    "                ]\n",
    "            ],\n",
    "            on=COLUMN_NAMES[\"YEAR_MONTH_DAY_HOUR\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "\n",
    "        # --- 2. Build lagged features ---\n",
    "        X = pd.DataFrame()\n",
    "        for lag in range(1, lags + 1):\n",
    "            X[f\"wind_speed_lag_{lag}\"] = df[COLUMN_NAMES[\"WIND_SPEED\"]].shift(lag)\n",
    "            X[f\"wind_dir_lag_{lag}\"] = df[COLUMN_NAMES[\"WIND_DIRECTION\"]].shift(lag)\n",
    "\n",
    "        y = df[\"GHI_target\"]\n",
    "\n",
    "        # Drop missing\n",
    "        data = pd.concat([X, y], axis=1).dropna()\n",
    "        X = data.drop(columns=COLUMN_NAMES[\"GHI\"], axis=1)\n",
    "        y = data[COLUMN_NAMES[\"GHI\"]]\n",
    "\n",
    "        # --- 3. Single-lag regressions ---\n",
    "        r2_scores = []\n",
    "        for lag in range(1, lags + 1):\n",
    "            X_lag = data[[f\"wind_speed_lag_{lag}\", f\"wind_dir_lag_{lag}\"]]\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_lag, y)\n",
    "            y_pred = model.predict(X_lag)\n",
    "            r2_scores.append(r2_score(y, y_pred))\n",
    "\n",
    "        best_single_lag = int(np.argmax(r2_scores) + 1)\n",
    "\n",
    "        lag_ranking = (\n",
    "            pd.DataFrame({\"lag_hour\": list(range(1, lags + 1)), \"r2_score\": r2_scores})\n",
    "            .sort_values(by=\"r2_score\", ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # --- 4. Multi-lag via Lasso ---\n",
    "        lasso = LassoCV(alphas=[alpha], cv=5).fit(X, y)\n",
    "        coef_mask = np.abs(lasso.coef_) > 1e-6\n",
    "        best_multi_lags = list(X.columns[coef_mask])\n",
    "\n",
    "        # --- 5. Optional plot ---\n",
    "        if plot:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.plot(range(1, lags + 1), r2_scores, marker=\"o\")\n",
    "            plt.xlabel(\"Lag hour\")\n",
    "            plt.ylabel(\"R² score\")\n",
    "            plt.title(f\"Aux Station {idx + 1}: Predictive power of wind (speed+dir)\")\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        # --- 6. Save results ---\n",
    "        results[idx] = {\n",
    "            \"best_single_lag\": best_single_lag,\n",
    "            \"lag_ranking\": lag_ranking,\n",
    "            \"best_multi_lags\": best_multi_lags,\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38df4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   GHI_kJ/m2  DNI_kJ/m2  DHI_kJ/m2  wind_speed  Hour_sin  Hour_cos  Month_sin  \\\n",
      "0          0          0          0    1.919726  0.000000  1.000000        0.0   \n",
      "1          0          0          0    1.204601  0.258819  0.965926        0.0   \n",
      "2          0          0          0    1.460003  0.500000  0.866025        0.0   \n",
      "3          0          0          0    1.204601  0.707107  0.707107        0.0   \n",
      "4          0          0          0    2.737011  0.866025  0.500000        0.0   \n",
      "\n",
      "   Month_cos  wind_dir_sin  wind_dir_cos  \n",
      "0        1.0     -0.173648      0.984808  \n",
      "1        1.0     -0.500000      0.866025  \n",
      "2        1.0      0.173648      0.984808  \n",
      "3        1.0      0.342020      0.939693  \n",
      "4        1.0     -0.173648      0.984808  \n",
      "   GHI_kJ/m2  DNI_kJ/m2  DHI_kJ/m2  wind_speed  wind_dir_sin  wind_dir_cos  \\\n",
      "0          0          0          0   -0.272811      0.829075      0.559137   \n",
      "1          0          0          0   -0.619139     -0.587840     -0.808977   \n",
      "2          0          0          0   -0.272811      0.913573      0.406675   \n",
      "3          0          0          0   -0.619139     -0.587840     -0.808977   \n",
      "4          0          0          0    0.477566      0.829075      0.559137   \n",
      "\n",
      "     distanceX    distanceY  \n",
      "0 -7977.193136  5796.591526  \n",
      "1 -7977.193136  5796.591526  \n",
      "2 -7977.193136  5796.591526  \n",
      "3 -7977.193136  5796.591526  \n",
      "4 -7977.193136  5796.591526  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Year Month Day Hour (YYYYMMDDHH)'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# wanted_df has: timestamp, GHI_target\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# aux_list = [aux1_df, aux2_df, aux3_df, aux4_df], \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results = \u001b[43manalyze_auxiliary_stations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwanted_station_data_dfs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_stations_dfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Show best lags\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m results.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36manalyze_auxiliary_stations\u001b[39m\u001b[34m(wanted_df, aux_list, lags, alpha, plot)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(aux_df.head())\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# --- 1. Merge wanted + auxiliary station on timestamp ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m df = pd.merge(\u001b[43mwanted_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCOLUMN_NAMES\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYEAR_MONTH_DAY_HOUR\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCOLUMN_NAMES\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGHI\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m     39\u001b[39m               aux_df[[COLUMN_NAMES[\u001b[33m\"\u001b[39m\u001b[33mYEAR_MONTH_DAY_HOUR\u001b[39m\u001b[33m\"\u001b[39m],COLUMN_NAMES[\u001b[33m\"\u001b[39m\u001b[33mWIND_SPEED\u001b[39m\u001b[33m\"\u001b[39m],COLUMN_NAMES[\u001b[33m\"\u001b[39m\u001b[33mWIND_DIRECTION\u001b[39m\u001b[33m\"\u001b[39m]]],\n\u001b[32m     40\u001b[39m               on=COLUMN_NAMES[\u001b[33m\"\u001b[39m\u001b[33mYEAR_MONTH_DAY_HOUR\u001b[39m\u001b[33m\"\u001b[39m], how=\u001b[33m'\u001b[39m\u001b[33minner\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# --- 2. Build lagged features ---\u001b[39;00m\n\u001b[32m     43\u001b[39m X = pd.DataFrame()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\super\\anaconda\\envs\\GHI\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\super\\anaconda\\envs\\GHI\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\super\\anaconda\\envs\\GHI\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['Year Month Day Hour (YYYYMMDDHH)'] not in index\""
     ]
    }
   ],
   "source": [
    "# wanted_df has: timestamp, GHI_target\n",
    "# aux_list = [aux1_df, aux2_df, aux3_df, aux4_df],\n",
    "results = analyze_auxiliary_stations(wanted_station_data_dfs[0], aux_stations_dfs)\n",
    "\n",
    "# Show best lags\n",
    "for i, res in results.items():\n",
    "    print(f\"\\nAux Station {i + 1}\")\n",
    "    print(\"  Best single lag:\", res[\"best_single_lag\"])\n",
    "    print(\"  Top 3 lag hours:\\n\", res[\"lag_ranking\"].head(3))\n",
    "    print(\"  Best multi-hour combo:\", res[\"best_multi_lags\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in aux_stations_dfs:\n",
    "    df[\"GHI_kJ/m2\"] = wanted_station_data_dfs[0][\"GHI_kJ/m2\"].values\n",
    "\n",
    "aux_stations_df = pd.concat(aux_stations_dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d39e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set: 694.38 W/m^2\n",
      "Feature importances:\n",
      "Wind speed / 0.1 m/s    0.458712\n",
      "alignment               0.189144\n",
      "wind_u                  0.148203\n",
      "distance                0.116659\n",
      "wind_v                  0.087282\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = engineer_features(aux_stations_df.copy())\n",
    "X, y = prepare_features(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "model = train_distance_model(X_train, y_train)\n",
    "evaluate_distance_model(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GHI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
